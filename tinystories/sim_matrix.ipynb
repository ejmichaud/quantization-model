{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M', cache_dir=\"data/\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\", cache_dir=\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/user/ericjm/miniconda3/envs/phase-changes/lib/python3.8/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_from_disk(\"data/tinystories_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_indexes = np.array([0] + list(np.cumsum(dataset[\"preds_len\"])))\n",
    "def loss_idx_to_dataset_idx(idx):\n",
    "    \"\"\"given an idx in range(0, 10658635), return\n",
    "    a sample index in range(0, 20000) and pred-in-sample\n",
    "    index in range(0, 1023). Note token-in-sample idx is\n",
    "    exactly pred-in-sample + 1\"\"\"\n",
    "    sample_index = np.searchsorted(starting_indexes, idx, side=\"right\") - 1\n",
    "    pred_in_sample_index = idx - starting_indexes[sample_index]\n",
    "    return int(sample_index), int(pred_in_sample_index)\n",
    "\n",
    "def get_context(idx):\n",
    "    \"\"\"given idx in range(0, 10658635), return dataset sample\n",
    "    and predicted token index within sample, in range(1, 1024).\"\"\"\n",
    "    sample_index, pred_index = loss_idx_to_dataset_idx(idx)\n",
    "    return dataset[sample_index], pred_index+1\n",
    "\n",
    "def print_context(idx):\n",
    "    \"\"\"\n",
    "    given idx in range(0, 10658635), print prompt preceding the corresponding\n",
    "    prediction, and highlight the predicted token.\n",
    "    \"\"\"\n",
    "    sample, token_idx = get_context(idx)\n",
    "    prompt = sample[\"split_by_token\"][:token_idx]\n",
    "    prompt = \"\".join(prompt)\n",
    "    token = sample[\"split_by_token\"][token_idx]\n",
    "    print(prompt + \"\\033[41m\" + token + \"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.load(\"data/losses.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowloss_idxs = (losses < 0.693).nonzero().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [n for n, _ in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "highsignal_names = [name for name in param_names if \n",
    "                        ('ln' not in name) and \n",
    "                        ('wte' not in name) and\n",
    "                        ('wpe' not in name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flattened_gradient(model, param_subset):\n",
    "    grads = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if name in param_subset:\n",
    "            grads.append(p.grad)\n",
    "    return torch.cat([g.flatten() for g in grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idxs = lowloss_idxs[::100][:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_g = sum(model.state_dict()[name].numel() for name in highsignal_names)\n",
    "S = len(token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_len = 200\n",
    "blocks = [token_idxs[i:min(len(token_idxs), i+block_len)] for i in range(0, len(token_idxs), block_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.zeros((S, S), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6236959114a43a8a6290a43be85d034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "outer loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iouter = 0\n",
    "for iblock in tqdm(blocks, desc=\"outer loop\"):\n",
    "    Gi = torch.zeros((len(iblock), len_g), device=device)\n",
    "    for i, idx in enumerate(iblock):\n",
    "        model.zero_grad()\n",
    "        document, l = get_context(idx)\n",
    "        prompt = document['text']\n",
    "        tokens = tokenizer(prompt, return_tensors='pt', max_length=1024, truncation=True).to(device)\n",
    "        logits = model(**tokens).logits\n",
    "        targets = tokens.input_ids\n",
    "        ls = torch.nn.functional.cross_entropy(logits[0, :-1, :], targets[0, 1:], reduction='none')\n",
    "        ls_l = ls[l-1]\n",
    "        ls_l.backward()\n",
    "        g = get_flattened_gradient(model, highsignal_names)\n",
    "        Gi[i] = g\n",
    "    Gi = F.normalize(Gi, p=2, dim=1)\n",
    "    j_index = blocks.index(iblock)\n",
    "    jouter = sum(len(block) for block in blocks[:j_index])\n",
    "    for jblock in tqdm(blocks[j_index:], leave=False, desc=\"inner loop\", display=False):\n",
    "        Gj = torch.zeros((len(jblock), len_g), device=device)\n",
    "        for j, idx in enumerate(jblock):\n",
    "            model.zero_grad()\n",
    "            document, l = get_context(idx)\n",
    "            prompt = document['text']\n",
    "            tokens = tokenizer(prompt, return_tensors='pt', max_length=1024, truncation=True).to(device)\n",
    "            logits = model(**tokens).logits\n",
    "            targets = tokens.input_ids\n",
    "            ls = torch.nn.functional.cross_entropy(logits[0, :-1, :], targets[0, 1:], reduction='none')\n",
    "            ls_l = ls[l-1]\n",
    "            ls_l.backward()\n",
    "            g = get_flattened_gradient(model, highsignal_names)\n",
    "            Gj[j] = g\n",
    "        Gj = F.normalize(Gj, p=2, dim=1)\n",
    "        Cij = torch.matmul(Gi, Gj.T)\n",
    "        C[iouter:iouter+len(iblock), jouter:jouter+len(jblock)] = Cij\n",
    "        C[jouter:jouter+len(jblock), iouter:iouter+len(iblock)] = Cij.T\n",
    "        jouter += len(jblock)\n",
    "    iouter += len(iblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((token_idxs, C), \"data/C-2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phase-changes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
