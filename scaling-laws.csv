Paper,Date,Category,Task,Task details,Architecture,Loss,Dependent Variable,Scaling Variable 1,Scaling Variable 2,Type,Functional form,Exponent 1,Exponent 2,Irreducible loss,Parameter values,Compute range (FLOP),,Data range (mixed),,Data unit,Size range (parameters),,Additional Conditions,Scaling strategy
A Constructive Prediction of the Generalization Error Across Scales,20 Dec 2019,Language,Language modeling,PTB,AWD-LSTM,Cross-entropy,Loss,Parameters,Data,Bivariate power law with transition - sum,"L(N,D) = I * ||(N^-a + BD^-b + E)/(N^-a + BD^-b + E -in)||",0.81,0.34,4.864433812,"a=0.81, B=0.15, b=0.34, E=5.00, n=6.27, I=6.10",,,2.81E+04,9.00E+05,Words,4.88E+02,2.00E+07,,
A Constructive Prediction of the Generalization Error Across Scales,20 Dec 2019,Language,Language modeling,WikiText-2,AWD-LSTM,Cross-entropy,Loss,Parameters,Data,Bivariate power law with transition - sum,"L(N,D) = I * ||(N^-a + BD^-b + E)/(N^-a + BD^-b + E -in)||",1.01,0.22,4.923728324,"a=1.01, B=0.99, b=0.22, E=8.23, n=10.38, I=6.21",,,6.25E+04,2.00E+06,Words,4.88E+02,2.00E+07,,
A Constructive Prediction of the Generalization Error Across Scales,20 Dec 2019,Language,Language modeling,WikiText-103,Transformer-XL,Cross-entropy,Loss,Parameters,Data,Bivariate power law with transition - sum,"L(N,D) = I * ||(N^-a + BD^-b + E)/(N^-a + BD^-b + E -in)||",0.74,0.56,3.651407589,"a=0.74, B=0.33, b=0.56, E=9.04, n=16.34, I=6.60",,,3.13E+06,1.00E+08,Words,1.00E+04,4.10E+07,,
A Constructive Prediction of the Generalization Error Across Scales,20 Dec 2019,Vision,Image classification,ImageNet,ResNet-50,Top-1 error,Loss,Parameters,Data,Bivariate power law with transition - sum,"L(N,D) = I * ||(N^-a + BD^-b + E)/(N^-a + BD^-b + E -in)||",0.75,0.61,0.1962162162,"a=0.75, B=0.76, b=0.61, E=3.63, n=18.50, I=1",,,1.88E+04,1.20E+06,Labelled images,6.23E+03,2.55E+07,,
A Constructive Prediction of the Generalization Error Across Scales,20 Dec 2019,Vision,Image classification,CIFAR10,WRN-44-16,Top-1 error,Loss,Parameters,Data,Bivariate power law with transition - sum,"L(N,D) = I * ||(N^-a + BD^-b + E)/(N^-a + BD^-b + E -in)||",0.66,0.53,3.61E-15,"a=0.66, B=5.87e−2, b=0.53, E=7.14e−14, n=19.77, I=1",,,1.88E+03,6.00E+04,Labelled images,2.73E+03,4.48E+07,,
A Constructive Prediction of the Generalization Error Across Scales,20 Dec 2019,Vision,Image classification,CIFAR100,WRN-44-16,Top-1 error,Loss,Parameters,Data,Bivariate power law with transition - sum,"L(N,D) = I * ||(N^-a + BD^-b + E)/(N^-a + BD^-b + E -in)||",0.7,0.51,0.1024531025,"a=0.70, B=0.15, b=0.51, E=0.71, n=6.93, I=1",,,1.88E+03,6.00E+04,Labelled images,2.73E+03,1.12E+07,,
A Constructive Prediction of the Generalization Error Across Scales,20 Dec 2019,Vision,Image classification,DTD,WRN-44-16,Top-1 error,Loss,Parameters,Data,Bivariate power law with transition - sum,"L(N,D) = I * ||(N^-a + BD^-b + E)/(N^-a + BD^-b + E -in)||",0.4,1.16,1.49E-09,"a=0.40, B=4.30e−5, b=1.16, E=1.27e−09, n=0.85, I=1",,,1.76E+02,5.64E+03,Labelled images,2.73E+03,1.12E+07,,
A Constructive Prediction of the Generalization Error Across Scales,20 Dec 2019,Vision,Image classification,Airfract,WRN-44-16,Top-1 error,Loss,Parameters,Data,Bivariate power law with transition - sum,"L(N,D) = I * ||(N^-a + BD^-b + E)/(N^-a + BD^-b + E -in)||",1.1,0.83,4.57E-10,"a=1.10, B=3.47e−3, b=0.83, E=5.16e−10, n=1.13, I=1",,,3.13E+02,1.00E+04,Labelled images,2.73E+03,1.12E+07,,
A Constructive Prediction of the Generalization Error Across Scales,20 Dec 2019,Vision,Image classification,UFC101,WRN-44-16,Top-1 error,Loss,Parameters,Data,Bivariate power law with transition - sum,"L(N,D) = I * ||(N^-a + BD^-b + E)/(N^-a + BD^-b + E -in)||",0.93,0.54,3.89E-10,"a=0.93, B=4.68e−2, b=0.54, E=1.16e−09, n=2.98, I=1",,,4.06E+02,1.30E+04,Labelled images,2.73E+03,1.12E+07,,
Scaling Laws for Neural Language Models,23 Jan 2020,Language,Language modeling,,Decoder-only Transformer,Cross-entropy,Loss,Compute,,Power law,L(C) = KC^-c,0.05,,,"K=26.38, c=0.05",1.69E+10,3.45E+19,2.20E+07,2.30E+10,BPE Tokens,7.68E+02,1.50E+09,,
Scaling Laws for Neural Language Models,23 Jan 2020,Language,Language modeling,,Decoder-only Transformer,Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^-a,0.076,,,"A=11.48, a=0.076",1.69E+10,3.45E+19,2.20E+07,2.30E+10,BPE Tokens,7.68E+02,1.50E+09,,
Scaling Laws for Neural Language Models,23 Jan 2020,Language,Language modeling,,Decoder-only Transformer,Cross-entropy,Loss,Data,,Power law,L(D) = BD^-b,0.095,,,"B=20.81, b=0.095",1.69E+10,3.45E+19,2.20E+07,2.30E+10,BPE Tokens,7.68E+02,1.50E+09,,
Scaling Laws for Neural Language Models,23 Jan 2020,Language,Language modeling,,Decoder-only Transformer,Cross-entropy,Loss,Parameters,Data,Bivariate power law,"L(N,D) = [(A/N)^(a/b) + B/D]^-b",0.076,0.095,,"A=11.48, a=0.076, B=20.81, b=0.095",1.69E+10,3.45E+19,2.20E+07,2.30E+10,BPE Tokens,7.68E+02,1.50E+09,,
Scaling Laws for Neural Language Models,23 Jan 2020,Language,Language modeling,,Decoder-only Transformer,Cross-entropy,Loss,Parameters,Training Steps,Bivariate power law - sum,"L(N,S) = AN^-a + BS^-b",0.076,0.76,,"A=11.48, a=0.076, B=334.88, b=0.76",1.69E+10,3.45E+19,2.20E+07,2.30E+10,BPE Tokens,7.68E+02,1.50E+09,Training at the critical batch size,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Language,Language modeling,,Decoder-only Transformer,Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^-a,0.07,,,"A=9.810, a=0.07",8.64E+14,4.32E+23,,,BPE Tokens,1.00E+05,1.75E+11,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Language,Language modeling,,Decoder-only Transformer,Cross-entropy,Loss,Compute,,Power law,L(C) = KC^-c,0.048,,,"K=23.27, c=0.048",8.64E+14,4.32E+23,,,BPE Tokens,1.00E+05,1.75E+11,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"8x8 Image modeling, pixel encoding",Decoder-only Transformer,Cross-entropy,Loss,Parameters,,Power law plus constant,L(N) = AN^-a + E,0.24,,3.12,"A=2.862, a=0.24",1.00E+13,8.64E+19,,1.00E+08,8x8 Images,1.00E+05,3.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"8x8 Image modeling, pixel encoding",Decoder-only Transformer,Cross-entropy,Loss,Compute,,Power law plus constant,L(C) = KC^-c + E,0.19,,3.13,"K=207.2, c=0.19",1.00E+13,8.64E+19,,1.00E+08,8x8 Images,1.00E+05,3.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"16x16 Image modeling, pixel encoding",Decoder-only Transformer,Cross-entropy,Loss,Parameters,,Power law plus constant,L(N) = AN^-a + E,0.22,,2.64,"A=3.454, a=0.22",1.73E+14,8.64E+20,,1.00E+08,16x16 Images,1.00E+05,2.00E+09,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"16x16 Image modeling, pixel encoding",Decoder-only Transformer,Cross-entropy,Loss,Compute,,Power law plus constant,L(C) = KC^-c + E,0.16,,2.64,"K=87.59, c=0.16",1.73E+14,8.64E+20,,1.00E+08,16x16 Images,1.00E+05,2.00E+09,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"32x32 Image modeling, pixel encoding",Decoder-only Transformer,Cross-entropy,Loss,Parameters,,Power law plus constant,L(N) = AN^-a + E,0.13,,2.2,"A=1.713, a=0.13",1.73E+14,8.64E+20,,1.00E+08,32x32 Images,1.00E+05,2.00E+09,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"32x32 Image modeling, pixel encoding",Decoder-only Transformer,Cross-entropy,Loss,Compute,,Power law plus constant,L(C) = KC^-c + E,0.1,,2.21,"K=14.10, c=0.1",1.73E+14,8.64E+20,,1.00E+08,32x32 Images,1.00E+05,2.00E+09,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"16x16 Image modeling, VQ encoding",Decoder-only Transformer,Cross-entropy,Loss,Parameters,,Power law plus constant,L(N) = AN^-a + E,0.13,,3.99,"A=3.767, a=0.13",1.00E+13,1.73E+20,,1.00E+08,64x64 VQ256 Images,1.00E+05,3.00E+09,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"16x16 Image modeling, VQ encoding",Decoder-only Transformer,Cross-entropy,Loss,Compute,,Power law plus constant,L(C) = KC^-c + E,0.11,,4.09,"K=32.31, c=0.11",1.00E+13,1.73E+20,,1.00E+08,64x64 VQ256 Images,1.00E+05,3.00E+09,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"32x32 Image modeling, VQ encoding",Decoder-only Transformer,Cross-entropy,Loss,Parameters,,Power law plus constant,L(N) = AN^-a + E,0.14,,3.07,"A=3.972, a=0.14",3.46E+14,2.59E+20,,1.00E+08,64x64 VQ1024 Images,1.00E+05,3.00E+09,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Vision,Image modeling,"32x32 Image modeling, VQ encoding",Decoder-only Transformer,Cross-entropy,Loss,Compute,,Power law plus constant,L(C) = KC^-c + E,0.12,,3.17,"K=52.74, c=0.12",3.46E+14,2.59E+20,,1.00E+08,64x64 VQ1024 Images,1.00E+05,3.00E+09,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Multimodal,Text-to-Image generation,,Decoder-only Transformer,Cross-entropy (text),Loss,Parameters,,Power law,L(N) = AN^-a,0.037,,,"A=2.107, a=0.037",1.73E+16,3.46E+20,,,"Captions (32x32 Image, 128 BPE token)",1.00E+05,8.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Multimodal,Text-to-Image generation,,Decoder-only Transformer,Cross-entropy (image),Loss,Parameters,,Power law plus constant,L(N) = AN^-a + E,0.16,,2,"A=3.919, a=0.16",1.73E+16,3.46E+20,,,"Captions (32x32 Image, 128 BPE token)",1.00E+05,8.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Multimodal,Text-to-Image generation,,Decoder-only Transformer,Cross-entropy (combined),Loss,Compute,,Power law plus constant,L(C) = KC^-c + E,0.15,,1.93,"K=130.8, c=0.15",1.73E+16,3.46E+20,,,"Captions (32x32 Image, 128 BPE token)",1.00E+05,8.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Multimodal,Image captioning,,Decoder-only Transformer,Cross-entropy (text),Loss,Parameters,,Power law,L(N) = AN^-a,0.039,,,"A=2.212, a=0.039",2.59E+16,3.46E+19,,,"Captions (32x32 Image, 128 BPE token)",1.00E+05,1.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Multimodal,Image captioning,,Decoder-only Transformer,Cross-entropy (image),Loss,Parameters,,Power law plus constant,L(N) = AN^-a + E,0.15,,2,"A=3.639, a=0.15",2.59E+16,3.46E+19,,,"Captions (32x32 Image, 128 BPE token)",1.00E+05,1.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Multimodal,Image captioning,,Decoder-only Transformer,Cross-entropy (combined),Loss,Compute,,Power law plus constant,L(C) = KC^-c + E,0.16,,1.97,"K=181.1, c=0.16",2.59E+16,3.46E+19,,,"Captions (32x32 Image, 128 BPE token)",1.00E+05,1.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Video,Video generation,,Decoder-only Transformer,Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^-a,0.24,,1.01,"A=12.48, a=0.24",8.64E+13,4.32E+20,,1.00E+02,64x64 VQ256 Video hours,1.00E+04,8.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Video,Video generation,,Decoder-only Transformer,Cross-entropy,Loss,Compute,,Power law,L(C) = KC^-c,0.14,,0.95,"K=137.7, c=0.14",8.64E+13,4.32E+20,,1.00E+02,64x64 VQ256 Video hours,1.00E+04,8.00E+08,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Language,Mathematics,,Decoder-only Transformer,Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^-a,0.16,,0.28,"A=4.432, a=0.16",2.59E+14,4.32E+20,,,Characters (bytes),2.00E+05,3.00E+09,,
Scaling Laws for Autoregressive Generative Modeling,28 Oct 2020,Language,Mathematics,,Decoder-only Transformer,Cross-entropy,Loss,Compute,,Power law,L(C) = KC^-c,0.17,,0.14,"K=366.4, c=0.17",2.59E+14,4.32E+20,,,Characters (bytes),2.00E+05,3.00E+09,,
Training Compute-Optimal Large Language Models,29 Mar 2022,Language,Language modeling,,Decoder-only Transformer,Cross-entropy,Loss,Parameters,Data,Bivariate power law plus constant - sum,"L(N,D) = AN^-a + BD^-b + E",0.34,0.28,1.69,"A=406.4, a=0.34, B=410.7, b=0.28, E=1.69",6.00E+18,3.00E+21,1.00E+07,1.00E+09,BPE Tokens,2.00E+07,1.60E+10,,
Revisiting Neural Scaling Laws in Language and Vision,13 Sept 2022,"Language, Vision",Multiple,,Multiple,Multiple,Loss,Data,,Power law with transition,(L-E)/(I-L)^alpha = BD^-b,,,,,,,,,,,,,
Scaling Laws for a Multi-Agent Reinforcement Learning Model,29 Sept 2022,"Games, RL",ConnectFour,,AlphaZero,Player strength,Loss,Parameters,,Power law,L(N) = N^a,0.88,,,a=0.88,1.50E+11,3.00E+16,2.00E+01,1.00E+04,Training Steps,6.00E+02,2.00E+05,,Width scaling
Scaling Laws for a Multi-Agent Reinforcement Learning Model,29 Sept 2022,"Games, RL",ConnectFour,,AlphaZero,,Loss,Compute,,Power law,L(C) = C^c,0.55,,,c=0.55,1.50E+11,3.00E+16,2.00E+01,1.00E+04,Training Steps,6.00E+02,2.00E+05,,Width scaling
Scaling Laws for a Multi-Agent Reinforcement Learning Model,29 Sept 2022,"Games, RL",Pentago,,AlphaZero,Player strength,Loss,Parameters,,Power law,L(N) = N^a,0.87,,,a=0.87,4.00E+11,4.00E+16,2.00E+01,1.00E+04,Training Steps,2.00E+03,3.00E+05,,Width scaling
Scaling Laws for a Multi-Agent Reinforcement Learning Model,29 Sept 2022,"Games, RL",Pentago,,AlphaZero,,Loss,Compute,,Power law,L(C) = C^c,0.55,,,c=0.55,4.00E+11,4.00E+16,2.00E+01,1.00E+04,Training Steps,2.00E+03,3.00E+05,,Width scaling
A Scaling Law for Syn2real Transfer: How Much Is Your Pre-training Effective?,25 Aug 2021,"Vision, Transfer",Multiple,,ResNet,Finetuning loss,Loss,Pretraining Data,,Power law plus constant,L(D) = BD^-b + G,,,,,,,,,,,,,
Effect of scale on catastrophic forgetting in neural networks,21 Sep 2022,Vision,,,,,,,,,,,,,,,,,,,,,,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=1.969, E=0.057, b=0.285",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Hybrid Transformer-LSTM,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=1.817, E=0.11, b=0.285",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Decoder-only Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.011, E=0.078, b=0.285",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.222, E=0.067, b=0.296",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,Source noise,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.772, E=0.323, b=0.296",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,Target noise,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.501, E=0.034, b=0.278",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,No filtering,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.235, E=0.054, b=0.278",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,CDS filtering,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.130, E=0.064, b=0.278",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,Bicleaner filtering,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.343, E=0.059, b=0.198",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,Back-translation 2L2L,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.288, E=0.054, b=0.198",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,Back-translation 6L6L,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.251, E=0.040, b=0.198",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,Back-translation 32L6L,
Data Scaling Laws in NMT: The Effect of Noise and Architecture,4 Feb 2022,Language,Translation,English to German,Encoder-Decoder Transformer,Cross-entropy,Loss,Data,,Power law plus constant,L(D) = B(D^-1 + E)^b,,,,"B=2.224, E=0.037, b=0.198",,,5.12E+05,5.12E+08,Sentence pairs,3.00E+08,3.00E+08,Back-translation 64L6L,
Scaling Laws for Reward Model Overoptimization,19 Oct 2022,RL,,,,,,,,,,,,,,,,,,,,,,
Beyond neural scaling laws: beating power law scaling via data pruning,15 Nov 2022,"Vision, Classification",Image classification,,ResNet,Test error,Loss,Data,,Exponential,L(D) = B exp(-bD),,,,,,,,,,,,Increasingly aggresive data pruning with a perfect pruning metric,
Transcending Scaling Laws with 0.1% Extra Compute,20 Oct 2022,Language,,,,,,,,,,,,,,,,,,,,,,
Learning Curve Theory,8 Feb 2021,Theory,,,,,,,,,,,,,,,,,,,,,,
Explaining Neural Scaling Laws,12 Feb 2021,Theory,,,,,,,,,,,,,,,,,,,,,,
Scaling Laws for Deep Learning,17 Aug 2021,Theory,,,,,,,,,,,,,,,,,,,,,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Encoder-Decoder Transformer,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.54,,,,1.21E+12,6.38E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Encoder-Decoder Transformer,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,0.28,,,,1.21E+12,6.38E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Encoder-Decoder Transformer,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.47,,,,1.21E+12,6.38E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Encoder-Decoder Transformer,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.24,,,,1.21E+12,6.38E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Encoder-Decoder Transformer,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,0.49,,,,1.21E+12,6.38E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Evolved Transformer,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.44,,,,1.31E+12,7.13E+13,,,,1.90E+07,2.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Evolved Transformer,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,0.22,,,,1.31E+12,7.13E+13,,,,1.90E+07,2.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Evolved Transformer,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.42,,,,1.31E+12,7.13E+13,,,,1.90E+07,2.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Evolved Transformer,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.21,,,,1.31E+12,7.13E+13,,,,1.90E+07,2.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Evolved Transformer,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,0.47,,,,1.31E+12,7.13E+13,,,,1.90E+07,2.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Universal Transformer,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.5,,,,1.77E+12,2.76E+13,,,,1.10E+07,2.83E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Universal Transformer,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,0.2,,,,1.77E+12,2.76E+13,,,,1.10E+07,2.83E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Universal Transformer,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.56,,,,1.77E+12,2.76E+13,,,,1.10E+07,2.83E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Universal Transformer,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.22,,,,1.77E+12,2.76E+13,,,,1.10E+07,2.83E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Universal Transformer,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,0.35,,,,1.77E+12,2.76E+13,,,,1.10E+07,2.83E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Switch Transformer,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.23,,,,3.25E+12,4.33E+13,,,,1.74E+08,2.96E+10,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Switch Transformer,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,0.14,,,,3.25E+12,4.33E+13,,,,1.74E+08,2.96E+10,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Switch Transformer,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.13,,,,3.25E+12,4.33E+13,,,,1.74E+08,2.96E+10,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Switch Transformer,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.08,,,,3.25E+12,4.33E+13,,,,1.74E+08,2.96E+10,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Switch Transformer,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,0.58,,,,3.25E+12,4.33E+13,,,,1.74E+08,2.96E+10,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Performer,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.25,,,,1.14E+12,3.28E+13,,,,1.60E+07,7.39E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Performer,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,0.05,,,,1.14E+12,3.28E+13,,,,1.60E+07,7.39E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Performer,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.24,,,,1.14E+12,3.28E+13,,,,1.60E+07,7.39E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Performer,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.05,,,,1.14E+12,3.28E+13,,,,1.60E+07,7.39E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Performer,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,0.24,,,,1.14E+12,3.28E+13,,,,1.60E+07,7.39E+08,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Funnel Transformer,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.47,,,,1.10E+12,4.03E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Funnel Transformer,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,0.22,,,,1.10E+12,4.03E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Funnel Transformer,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.38,,,,1.10E+12,4.03E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Funnel Transformer,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.18,,,,1.10E+12,4.03E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,Funnel Transformer,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,0.46,,,,1.10E+12,4.03E+13,,,,1.60E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,ALBERT,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.08,,,,3.57E+12,3.16E+13,,,,1.50E+07,3.40E+07,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,ALBERT,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,-0.12,,,,3.57E+12,3.16E+13,,,,1.50E+07,3.40E+07,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,ALBERT,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.13,,,,3.57E+12,3.16E+13,,,,1.50E+07,3.40E+07,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,ALBERT,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,-0.21,,,,3.57E+12,3.16E+13,,,,1.50E+07,3.40E+07,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,ALBERT,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,-1.67,,,,3.57E+12,3.16E+13,,,,1.50E+07,3.40E+07,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MoS-Transformer,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.43,,,,1.29E+12,1.12E+14,,,,2.70E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MoS-Transformer,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,0.21,,,,1.29E+12,1.12E+14,,,,2.70E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MoS-Transformer,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.43,,,,1.29E+12,1.12E+14,,,,2.70E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MoS-Transformer,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.2,,,,1.29E+12,1.12E+14,,,,2.70E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MoS-Transformer,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,0.47,,,,1.29E+12,1.12E+14,,,,2.70E+07,2.90E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,GLU-Transformer,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.49,,,,1.29E+12,6.13E+13,,,,2.60E+07,2.85E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,GLU-Transformer,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,0.24,,,,1.29E+12,6.13E+13,,,,2.60E+07,2.85E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,GLU-Transformer,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.42,,,,1.29E+12,6.13E+13,,,,2.60E+07,2.85E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,GLU-Transformer,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.22,,,,1.29E+12,6.13E+13,,,,2.60E+07,2.85E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,GLU-Transformer,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,0.46,,,,1.29E+12,6.13E+13,,,,2.60E+07,2.85E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,LConv,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.32,,,,1.20E+12,7.70E+13,,,,1.70E+07,2.30E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,LConv,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,0.13,,,,1.20E+12,7.70E+13,,,,1.70E+07,2.30E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,LConv,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.29,,,,1.20E+12,7.70E+13,,,,1.70E+07,2.30E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,LConv,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.11,,,,1.20E+12,7.70E+13,,,,1.70E+07,2.30E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,LConv,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,0.48,,,,1.20E+12,7.70E+13,,,,1.70E+07,2.30E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,DConv,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,,,,,1.39E+12,7.80E+13,,,,2.20E+07,1.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,DConv,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,,,,,1.39E+12,7.80E+13,,,,2.20E+07,1.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,DConv,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,,,,,1.39E+12,7.80E+13,,,,2.20E+07,1.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,DConv,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,,,,,1.39E+12,7.80E+13,,,,2.20E+07,1.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,DConv,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,,,,,1.39E+12,7.80E+13,,,,2.20E+07,1.20E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MLP-Mixer,Upstream Negative Cross-entropy,Loss,Compute,,Power law,L(C) = KC^c,0.32,,,,3.83E+12,4.83E+13,,,,6.70E+07,2.86E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MLP-Mixer,Downstream Accuracy,Loss,Compute,,Power law,L(C) = KC^c,-0.03,,,,3.83E+12,4.83E+13,,,,6.70E+07,2.86E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MLP-Mixer,Upstream Negative Cross-entropy,Loss,Parameters,,Power law,L(N) = AN^a,0.26,,,,3.83E+12,4.83E+13,,,,6.70E+07,2.86E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MLP-Mixer,Downstream Accuracy,Loss,Parameters,,Power law,L(N) = AN^a,0.65,,,,3.83E+12,4.83E+13,,,,6.70E+07,2.86E+09,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,21 Jul 2022,Language,Language modeling,,MLP-Mixer,Downstream Accuracy,Loss,Upstream Negative Cross-entropy,,Power law,Ld(Lu) = GLu^l,-0.02,,,,3.83E+12,4.83E+13,,,,6.70E+07,2.86E+09,,
Understanding Scaling Laws for Recommendation Models,17 Aug 2022,Recommendation,Click-Through Rate prediction,,DLRM,Normalized Cross-Entropy,Loss,Data,,Power law plus constant,L(D) = BD^-b + E,0.1,,0.97,"B=0.07, b=0.1, E=0.98",1.00E+14,5.00E+18,5.00E+06,5.12E+09,Datapoints,4.00E+06,5.00E+11,,
Understanding Scaling Laws for Recommendation Models,17 Aug 2022,Recommendation,Click-Through Rate prediction,,DLRM,Normalized Cross-Entropy,Loss,Parameters,,Power law plus constant,L(N) = AN^a + E,4,,0.98,"A=0.25, a=4, E=0.98",1.00E+14,5.00E+18,5.00E+06,5.12E+09,Datapoints,4.00E+06,5.00E+11,,Several
Understanding Scaling Laws for Recommendation Models,17 Aug 2022,Recommendation,Click-Through Rate prediction,,DLRM,Normalized Cross-Entropy,Loss,Compute,,Power law plus constant,L(C) = KC^c + E,0.14,,0.98,"K=0.11, c=0.14, E=0.98",1.00E+14,5.00E+18,5.00E+06,5.12E+09,Datapoints,4.00E+06,5.00E+11,,
Scaling Laws for Transfer,2 Feb 2021,"Transfer, Language",Python generation,Transfer from text,Decoder-only Transformer,Cross-entropy,Data Transferred,Parameters,Finetuning data,Bivariate power law - product,"Dt(Df, N) = AN^a Df^b",0.38,0.18,,"A=1.9e4, a=0.38, b=0.18",,,4.00E+06,2.00E+09,Characters,1.00E+05,1.00E+09,Pretraining until convergence,
Scaling Laws for Transfer,2 Feb 2021,"Transfer, Language",Python generation,Transfer from text+code,Decoder-only Transformer,Cross-entropy,Data Transferred,Parameters,Finetuning data,Bivariate power law - product,"Dt(Df, N) = AN^a Df^b",0.38,0.096,,"A=2.1e5, a=0.38, b=0.096",,,4.00E+06,9.00E+08,Characters,1.00E+05,2.00E+08,Pretraining until convergence,
Scaling Laws for Neural Machine Translation,16 Sep 2021,Language,Translation,En-De,Encoder-Decoder Transformer,Cross-entropy,Loss,Encoder Parameters,Decoder Parameters,Bivariate power law plus constant - product,"L(Ne, Nd) = ANe^-ae Nd^-ad + E",,,,,,,,,,1.91E+08,3.05E+09,,Depth scaling
Data and Parameter Scaling Laws for Neural Machine Translation,1 Nov 2021,Language,Translation,Ru-En,Encoder-Decoder Transformer,Cross-entropy,Loss,Parameters,Data,Bivariate power law,"L(N,D) = [AN^(-a/b) + BD^-1]^b",0.11,0.38,,"A=522.3, a=0.11, B=9.95E+05, b=0.38",,,4.00E+04,5.00E+07,Lines of text,3.93E+05,5.60E+07,,
Data and Parameter Scaling Laws for Neural Machine Translation,1 Nov 2021,Language,Translation,De-En,Encoder-Decoder Transformer,Cross-entropy,Loss,Parameters,Data,Bivariate power law,"L(N,D) = [AN^(-a/b) + BD^-1]^b",0.13,0.35,,"A=1082 , a=0.13, B=6.80E+05, b=0.35",,,4.00E+04,5.00E+07,Lines of text,3.93E+05,5.60E+07,,
Data and Parameter Scaling Laws for Neural Machine Translation,1 Nov 2021,Language,Translation,Zh-En,Encoder-Decoder Transformer,Cross-entropy,Loss,Parameters,Data,Bivariate power law,"L(N,D) = [AN^(-a/b) + BD^-1]^b",0.12,0.43,,"A=258.1, a=0.12, B=3.38E+05, b=0.43",,,4.00E+04,5.00E+07,Lines of text,3.93E+05,5.60E+07,,
Broken Neural Scaling Laws,16 Oct 2022,Multiple,,,Multiple,Multiple,Loss,Any,,Smoothly broken power law,L(D) = E + (BX^-b_0) Prod_i=1^n (1+(X/D_i)^(1/f_i))^(-b_i*f_i),,,,,,,,,,,,,
Scaling Vision Transformers,8 Jun 2022,"Vision, Transfer",ImageNet,Fine-tuning,Vision Transformer,Accuracy,Loss,Compute,,Power law with transition,L(C) = K(C+C0)^-c + E,0.35,,0.09,"K=0.26, C0=0.01, c=0.35, E=0.09",2.13E+18,1.06272E+23,1.00E+08,1.00E+10,Images,5.40E+06,1.80E+09,,
Scaling Vision Transformers,8 Jun 2022,"Vision, Transfer",ImageNet,Linear 10-shot,Vision Transformer,Accuracy,Loss,Compute,,Power law with transition,L(C) = K(C+C0)^-c + E,0.32,,0.12,"K=0.63, C0=0.52, c=0.32, E=0.12",2.13E+18,1.06272E+23,1.00E+08,1.00E+10,Images,5.40E+06,1.80E+09,,
Scaling Laws for Acoustic Models,11 Jun 2021,Speech,Speech modeling,,Decoder-only Transformer,,Loss,Parameters,,Power law plus constant,L(N) = AN^-a + E,0.016,,0.316,"A=0.41, a=0.016, E=0.316",4.32E+17,6.912E+19,1.00E+02,2.00E+04,Hours of speech,4.00E+05,2.00E+07,,
Scaling Laws for Acoustic Models,11 Jun 2021,Speech,Speech modeling,,Decoder-only Transformer,,Loss,Data,,Power law plus constant,L(D) = BD^-b + E,0.019,,0.316,"B=0.37, b=0.019, E=0.316",4.32E+17,6.912E+19,1.00E+02,2.00E+04,Hours of speech,4.00E+05,2.00E+07,,
Scaling Scaling Laws with Board Games,7 Apr 2021,"Games, RL",Hex,,AlphaZero,Elo,Loss,Compute,Board size,Logarithmic with transition,"L(D,Bs) = (Mi * Bs + K log C + ci).clamp(Mp * Bs + cp, 0)",,,,"Mi = -430, K=510, ci=-4400, Mp=-270, cp=570",1.00E+09,1.00E+17,,,,,,,